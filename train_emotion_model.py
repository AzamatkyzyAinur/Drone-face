import tensorflow as tf
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import ModelCheckpoint

from PIL import Image
import os

dataset_path = "dataset"  # –ü—É—Ç—å –∫ –ø–∞–ø–∫–µ —Å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º–∏

def check_images(directory):
    """–£–¥–∞–ª—è–µ—Ç –ø–æ–≤—Ä–µ–∂–¥–µ–Ω–Ω—ã–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –∏ –ø—Ä–æ–ø—É—Å–∫–∞–µ—Ç –ø–∞–ø–∫–∏."""
    for folder in os.listdir(directory):  
        folder_path = os.path.join(directory, folder)
        if not os.path.isdir(folder_path):  # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Ñ–∞–π–ª—ã, –µ—Å–ª–∏ —ç—Ç–æ –Ω–µ –ø–∞–ø–∫–∞
            continue
        
        for file in os.listdir(folder_path):
            file_path = os.path.join(folder_path, file)
            
            if not os.path.isfile(file_path):  # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –ø–∞–ø–∫–∏
                continue

            try:
                with Image.open(file_path) as img:
                    img.verify()  # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
            except Exception as e:
                print(f"[!] –£–¥–∞–ª—è—é –ø–æ–≤—Ä–µ–∂–¥–µ–Ω–Ω–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ: {file_path}, –û—à–∏–±–∫–∞: {e}")
                os.remove(file_path)  # –£–¥–∞–ª—è–µ–º —Ç–æ–ª—å–∫–æ —Ñ–∞–π–ª—ã

# ‚úÖ –ó–∞–ø—É—Å–∫–∞–µ–º –ø—Ä–æ–≤–µ—Ä–∫—É –ø–µ—Ä–µ–¥ –∑–∞–≥—Ä—É–∑–∫–æ–π –¥–∞—Ç–∞—Å–µ—Ç–∞
print("üîç –ü—Ä–æ–≤–µ—Ä—è–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è...")
check_images(dataset_path)
print("‚úÖ –ü—Ä–æ–≤–µ—Ä–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞!\n")



# –ê—É–≥–º–µ–Ω—Ç–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö
train_datagen = ImageDataGenerator(
    rescale=1./255,
    rotation_range=30,
    width_shift_range=0.3,
    height_shift_range=0.3,
    shear_range=0.3,
    zoom_range=0.3,
    horizontal_flip=True,
    fill_mode='nearest',
    validation_split=0.2
)

test_datagen = ImageDataGenerator(rescale=1./255, validation_split=0.2)

# –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ
train_generator = train_datagen.flow_from_directory(
    'dataset/',
    target_size=(224, 224),
    batch_size=32,
    class_mode='categorical',
    subset='training'
)

val_generator = test_datagen.flow_from_directory(
    'dataset/',
    target_size=(224, 224),
    batch_size=32,
    class_mode='categorical',
    subset='validation'
)

# –°–æ–∑–¥–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏
model = Sequential([
    Conv2D(32, (3,3), activation='relu', input_shape=(224, 224, 3)),
    MaxPooling2D(pool_size=(2,2)),
    
    Conv2D(64, (3,3), activation='relu'),
    MaxPooling2D(pool_size=(2,2)),
    
    Flatten(),
    Dense(128, activation='relu'),
    Dropout(0.5),  # –£–±–∏—Ä–∞–µ–º 50% —Å–ª—É—á–∞–π–Ω—ã—Ö –Ω–µ–π—Ä–æ–Ω–æ–≤
    Dense(train_generator.num_classes, activation='softmax')
])

# –ö–æ–º–ø–∏–ª—è—Ü–∏—è –º–æ–¥–µ–ª–∏
model.compile(
    optimizer=Adam(learning_rate=0.0001),
    loss='categorical_crossentropy',
    metrics=['accuracy']
)

# –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –ª—É—á—à–µ–π –º–æ–¥–µ–ª–∏
checkpoint = ModelCheckpoint(
    "best_model.h5",
    monitor="val_accuracy",
    save_best_only=True,
    mode="max",
    verbose=1
)

# –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
model.fit(
    train_generator,
    validation_data=val_generator,
    epochs=20,
    callbacks=[checkpoint]
)

# –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∏—Ç–æ–≥–æ–≤–æ–π –º–æ–¥–µ–ª–∏
model.save("emotion_classification_model.h5")
print("‚úÖ –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –∫–∞–∫ 'emotion_classification_model.h5'")
